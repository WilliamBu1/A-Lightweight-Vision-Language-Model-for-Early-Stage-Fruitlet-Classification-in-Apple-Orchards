{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d50c73a-93ed-426b-82a3-dedf9907adc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=1.21.0'\n",
      "'=4.21.0'\n",
      "'=8.0.0'\n",
      " calibration_data\n",
      " clip_assets.pt\n",
      " create_deployment_assets.py\n",
      " cuda-tegra-repo-ubuntu2204-12-9-local_12.9.1-1_arm64.deb\n",
      " cudatest.py\n",
      " final_eval.py\n",
      " finetuned_tinyclip_multilabel.pt\n",
      " install_cusparselt.sh\n",
      " install_cusparselt.sh.1\n",
      " int8_calibration.cache\n",
      " libcusparse_lt-linux-sbsa-0.5.2.1-archive\n",
      " libcusparse_lt-linux-sbsa-0.5.2.1-archive.tar.xz\n",
      " onnx2trt.py\n",
      " requirements.txt\n",
      " sliding_window.ipynb\n",
      " tinyclip_dynamic.onnx\n",
      " tinyclip_fp16_dynamic.trt\n",
      " tinyclip_fp16.trt\n",
      " tinyclip_int8_dynamic.trt\n",
      " tinyclip_int8.trt\n",
      " tinyCLIPval.py\n",
      " tinyclip_vision_model_complete.onnx\n",
      " tmp_cusparselt\n",
      " torch-1.10.0-cp38-cp38-linux_aarch64.whl\n",
      " torch-1.13.0-cp38-cp38-linux_aarch64.whl\n",
      " torch-1.8.0-cp36-cp36m-linux_aarch64.whl\n",
      " torch-2.0.0-cp38-cp38-linux_aarch64.whl\n",
      " torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl\n",
      " torch-2.1.0a0+41361538.nv23.06-cp38-cp38-linux_aarch64.whl\n",
      " torch-2.1.0-cp38-cp38-linux_aarch64.whl\n",
      " trt_env\n",
      " valid\n",
      " valid_crops\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf41ea56-a006-4ee3-ab17-6728f25d2a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- IMPORTS ---\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# --- NEW: TensorRT and PyCUDA Imports ---\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit # This is needed for initializing CUDA driver\n",
    "\n",
    "# Suppress specific warnings from PIL about large images\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='PIL')\n",
    "\n",
    "# --- 1. CONFIGURATION (MODIFIED) ---\n",
    "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
    "# --- MODIFIED: Point to the TensorRT engine file ---\n",
    "SAVED_MODEL_PATH = 'tinyclip_int8_dynamic.trt'\n",
    "# --- MODIFIED: Use relative path for Jupyter environment ---\n",
    "VALIDATION_DIR = 'valid'\n",
    "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "\n",
    "# BATCH_SIZE for processing patches to prevent out-of-memory errors\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Sliding Window Parameters\n",
    "PATCH_SIZE = 224\n",
    "STRIDE = 112\n",
    "\n",
    "# We must use CUDA for TensorRT\n",
    "device = \"cuda\"\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ded88f78-ddd1-4b5c-8749-14086434bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import numpy as np\n",
    "\n",
    "class TensorRTInference:\n",
    "    def __init__(self, engine_path):\n",
    "        print(f\"Loading TensorRT engine from: {engine_path}\")\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "        \n",
    "        with open(engine_path, \"rb\") as f, trt.Runtime(self.logger) as runtime:\n",
    "            self.engine = runtime.deserialize_cuda_engine(f.read())\n",
    "        \n",
    "        if self.engine is None:\n",
    "            raise RuntimeError(\"Failed to load TensorRT engine\")\n",
    "            \n",
    "        self.context = self.engine.create_execution_context()\n",
    "        self.stream = cuda.Stream()\n",
    "\n",
    "        print(f\"Engine loaded successfully!\")\n",
    "        print(f\"Number of IO tensors: {self.engine.num_io_tensors}\")\n",
    "        \n",
    "        # Initialize input/output information\n",
    "        self.input_info = {}\n",
    "        self.output_info = {}\n",
    "        \n",
    "        for i in range(self.engine.num_io_tensors):\n",
    "            tensor_name = self.engine.get_tensor_name(i)\n",
    "            tensor_shape = self.engine.get_tensor_shape(tensor_name)\n",
    "            tensor_dtype = trt.nptype(self.engine.get_tensor_dtype(tensor_name))\n",
    "            tensor_mode = self.engine.get_tensor_mode(tensor_name)\n",
    "            \n",
    "            print(f\"Tensor '{tensor_name}': shape={tensor_shape}, dtype={tensor_dtype}\")\n",
    "            \n",
    "            if tensor_mode == trt.TensorIOMode.INPUT:\n",
    "                self.input_info[tensor_name] = {\n",
    "                    'shape': tensor_shape,\n",
    "                    'dtype': tensor_dtype\n",
    "                }\n",
    "                self.input_tensor_name = tensor_name  # Assuming single input\n",
    "                print(f\"  -> INPUT tensor\")\n",
    "            else:\n",
    "                self.output_info[tensor_name] = {\n",
    "                    'shape': tensor_shape,\n",
    "                    'dtype': tensor_dtype\n",
    "                }\n",
    "                self.output_tensor_name = tensor_name  # Assuming single output\n",
    "                print(f\"  -> OUTPUT tensor\")\n",
    "        \n",
    "        print(f\"Initialized {len(self.input_info)} inputs and {len(self.output_info)} outputs\")\n",
    "    \n",
    "    def infer(self, pixel_values: np.ndarray):\n",
    "        batch_size = pixel_values.shape[0]\n",
    "        \n",
    "        #print(f\"Running inference with batch_size={batch_size}\")\n",
    "        #print(f\"pixel_values shape: {pixel_values.shape}\")\n",
    "        \n",
    "        # Set the input shape for dynamic batching\n",
    "        actual_input_shape = list(pixel_values.shape)\n",
    "        #print(f\"Setting {self.input_tensor_name} shape to {actual_input_shape}\")\n",
    "        \n",
    "        # This is the key fix: set input shape before setting tensor addresses\n",
    "        self.context.set_input_shape(self.input_tensor_name, actual_input_shape)\n",
    "        \n",
    "        # Now allocate memory based on the actual shapes\n",
    "        input_nbytes = int(pixel_values.nbytes)\n",
    "        output_shape = self.context.get_tensor_shape(self.output_tensor_name)\n",
    "        output_size = int(np.prod(output_shape))\n",
    "        output_nbytes = int(output_size * np.dtype(self.output_info[self.output_tensor_name]['dtype']).itemsize)\n",
    "        \n",
    "        #print(f\"Output shape after setting input: {output_shape}\")\n",
    "        #print(f\"Allocating {input_nbytes} bytes for input, {output_nbytes} bytes for output\")\n",
    "        \n",
    "        # Allocate GPU memory\n",
    "        d_input = cuda.mem_alloc(input_nbytes)\n",
    "        d_output = cuda.mem_alloc(output_nbytes)\n",
    "        \n",
    "        # Set tensor addresses\n",
    "        self.context.set_tensor_address(self.input_tensor_name, int(d_input))\n",
    "        self.context.set_tensor_address(self.output_tensor_name, int(d_output))\n",
    "        \n",
    "        # Copy input data to GPU\n",
    "        cuda.memcpy_htod_async(d_input, pixel_values, self.stream)\n",
    "        \n",
    "        # Execute inference\n",
    "        if not self.context.execute_async_v3(stream_handle=self.stream.handle):\n",
    "            raise RuntimeError(\"TensorRT inference execution failed\")\n",
    "        \n",
    "        # Allocate host memory for output and copy back\n",
    "        output_host = np.empty(output_shape, dtype=self.output_info[self.output_tensor_name]['dtype'])\n",
    "        cuda.memcpy_dtoh_async(output_host, d_output, self.stream)\n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        # Clean up GPU memory\n",
    "        d_input.free()\n",
    "        d_output.free()\n",
    "        \n",
    "        #print(f\"Inference completed. Output shape: {output_host.shape}\")\n",
    "        return output_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "88ef2c8a-ebac-4a85-b78b-b3249058b968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading TENSORRT Vision Model for Stage 2 ---\n",
      "Loading TensorRT engine from: tinyclip_int8_dynamic.trt\n",
      "Engine loaded successfully!\n",
      "Number of IO tensors: 2\n",
      "Tensor 'input': shape=(-1, 3, 224, 224), dtype=<class 'numpy.float32'>\n",
      "  -> INPUT tensor\n",
      "Tensor 'output': shape=(-1, 512), dtype=<class 'numpy.float32'>\n",
      "  -> OUTPUT tensor\n",
      "Initialized 1 inputs and 1 outputs\n",
      "--- Encoding text prompts ONCE using PyTorch CLIP model ---\n",
      "Encoded text features shape: torch.Size([4, 512])\n",
      "Using device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. MODEL AND PROCESSOR SETUP (MODIFIED) ---\n",
    "print(\"--- Loading TENSORRT Vision Model for Stage 2 ---\")\n",
    "if not os.path.exists(SAVED_MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"TensorRT model not found at {SAVED_MODEL_PATH}.\")\n",
    "\n",
    "# Instantiate our TensorRT wrapper for the VISION model\n",
    "trt_model = TensorRTInference(SAVED_MODEL_PATH)\n",
    "\n",
    "# The processor is still needed for preprocessing images\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# --- NEW: Encode text prompts ONCE using PyTorch CLIP model ---\n",
    "print(\"--- Encoding text prompts ONCE using PyTorch CLIP model ---\")\n",
    "full_clip_model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "text_prompts = [f\"a photo of a {label}\" for label in CLASS_LABELS]\n",
    "text_inputs = processor(text=text_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "# Ensure to use the text model and text projection\n",
    "with torch.no_grad():\n",
    "    text_features = full_clip_model.get_text_features(input_ids=text_inputs['input_ids'], \n",
    "                                                      attention_mask=text_inputs['attention_mask'])\n",
    "# Normalize text features as done in CLIP\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True) \n",
    "print(f\"Encoded text features shape: {text_features.shape}\")\n",
    "print(f\"Using device: {device}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6154c124-afbb-4b75-b1aa-986c3e6d6ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Processing COCO Ground Truth ---\n",
      "Processed ground truth for 60 images.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. LOAD AND PROCESS GROUND TRUTH (Unchanged) ---\n",
    "print(\"--- Loading and Processing COCO Ground Truth ---\")\n",
    "annotation_file_path = os.path.join(VALIDATION_DIR, '_annotations.coco.json')\n",
    "if not os.path.exists(annotation_file_path):\n",
    "    raise FileNotFoundError(f\"Annotation file not found at {annotation_file_path}.\")\n",
    "\n",
    "with open(annotation_file_path, 'r') as f:\n",
    "    coco_data = json.load(f)\n",
    "\n",
    "# Create mappings and load all annotations\n",
    "coco_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
    "train_class_labels = [lbl for lbl in CLASS_LABELS if lbl != \"negative\"]\n",
    "name_to_class_idx = {name: i for i, name in enumerate(train_class_labels)}\n",
    "coco_id_to_class_idx = {\n",
    "    coco_id: name_to_class_idx.get(name) for coco_id, name in coco_id_to_name.items() if name in train_class_labels\n",
    "}\n",
    "\n",
    "image_id_to_filename = {img['id']: img['file_name'] for img in coco_data['images']}\n",
    "image_id_to_filename_subset = dict(list(image_id_to_filename.items())[:])\n",
    "image_id_to_annotations = {img_id: [] for img_id in image_id_to_filename}\n",
    "for ann in coco_data['annotations']:\n",
    "    image_id_to_annotations[ann['image_id']].append(ann)\n",
    "print(f\"Processed ground truth for {len(image_id_to_filename)} images.\\n\")\n",
    "\n",
    "\n",
    "# --- 4. HELPER FUNCTION (Unchanged) ---\n",
    "def get_patch_ground_truth(patch_box, image_annotations, overlap_threshold=0.1):\n",
    "    px1, py1, px2, py2 = patch_box\n",
    "    patch_area = (px2 - px1) * (py2 - py1)\n",
    "    patch_truth = [0] * len(train_class_labels)\n",
    "    for ann in image_annotations:\n",
    "        bbox = ann['bbox']\n",
    "        bx1, by1, bw, bh = bbox; bx2, by2 = bx1 + bw, by1 + bh\n",
    "        ix1, iy1 = max(px1, bx1), max(py1, by1)\n",
    "        ix2, iy2 = min(px2, bx2), min(py2, by2)\n",
    "        inter_area = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
    "        if (inter_area / patch_area) > overlap_threshold:\n",
    "            class_idx = coco_id_to_class_idx.get(ann['category_id'])\n",
    "            if class_idx is not None:\n",
    "                patch_truth[class_idx] = 1\n",
    "    is_negative = 1 if sum(patch_truth) == 0 else 0\n",
    "    return patch_truth + [is_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "751e648c-307b-476b-b34a-55f66dc04d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running OPTIMIZED Sliding Window Analysis on All 60 Images ---\n",
      "\n",
      "\n",
      "=========================================================\n",
      "Processing Image: IMG_2197_JPEG.rf.912aa4bfc6ecb5926978c385be331efe.jpg\n",
      "=============================================================\n",
      "Extracted 910 patches. Processing them in batches of 8...\n",
      "\n",
      "--- Processing batch 1 with 8 patches ---\n",
      "Preprocessed inputs keys: dict_keys(['pixel_values'])\n",
      "pixel_values shape: torch.Size([8, 3, 224, 224])\n",
      "NumPy pixel_values shape: (8, 3, 224, 224)\n",
      "pixel_values dtype: float32\n",
      "TensorRT output (image_features) shape: (8, 512)\n",
      "Batch processed successfully!\n",
      "Probabilities shape: torch.Size([8, 4])\n",
      "Predictions shape: torch.Size([8, 4])\n",
      "\n",
      "Final concatenated results:\n",
      "full_probs_tensor shape: torch.Size([8, 4])\n",
      "full_predictions_tensor shape: torch.Size([8, 4])\n",
      "--> Image processing finished in 0.40 seconds.\n",
      "\n",
      "\n",
      "--- Debug analysis complete. ---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# --- 5. OPTIMIZED SLIDING WINDOW INFERENCE LOOP ---\n",
    "print(f\"--- Running OPTIMIZED Sliding Window Analysis on All {len(image_id_to_filename_subset)} Images ---\")\n",
    "# Should be (batch_size, 3, H, W) and match what the engine expects\n",
    "\n",
    "for image_id, filename in list(image_id_to_filename_subset.items())[:1]:  # Process only first image for debugging\n",
    "    print(f\"\\n\\n=========================================================\")\n",
    "    print(f\"Processing Image: {filename}\")\n",
    "    print(f\"=============================================================\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    image_path = os.path.join(VALIDATION_DIR, filename)\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"--> SKIPPING: File not found at {image_path}\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_width, image_height = image.size\n",
    "    current_image_annotations = image_id_to_annotations[image_id]\n",
    "\n",
    "    patches = []\n",
    "    num_patches_y = (image_height - PATCH_SIZE) // STRIDE + 1\n",
    "    num_patches_x = (image_width - PATCH_SIZE) // STRIDE + 1\n",
    "\n",
    "    for y in range(0, image_height - PATCH_SIZE + 1, STRIDE):\n",
    "        for x in range(0, image_width - PATCH_SIZE + 1, STRIDE):\n",
    "            patch = image.crop((x, y, x + PATCH_SIZE, y + PATCH_SIZE))\n",
    "            patches.append(patch)\n",
    "\n",
    "    if not patches:\n",
    "        print(\"--> SKIPPING: No patches were generated for this image.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Extracted {len(patches)} patches. Processing them in batches of {BATCH_SIZE}...\")\n",
    "\n",
    "    all_probs = []\n",
    "    image_patch_predictions = []\n",
    "\n",
    "    # Process the list of patches in batches\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, min(BATCH_SIZE, len(patches)), BATCH_SIZE):  # Process only first batch for debugging\n",
    "            batch = patches[i:i + BATCH_SIZE]\n",
    "            print(f\"\\n--- Processing batch {i//BATCH_SIZE + 1} with {len(batch)} patches ---\")\n",
    "            \n",
    "            # 1. Preprocess images with Hugging Face processor\n",
    "            # The 'text' argument is NOT passed here, as text is pre-encoded\n",
    "            inputs = processor(images=batch, return_tensors=\"pt\").to(device) \n",
    "            \n",
    "            print(f\"Preprocessed inputs keys: {inputs.keys()}\")\n",
    "            print(f\"pixel_values shape: {inputs['pixel_values'].shape}\")\n",
    "            \n",
    "            # --- Run inference using the TensorRT wrapper (only pixel_values) ---\n",
    "            # 2. Convert PyTorch pixel_values tensor to NumPy array for the TRT engine\n",
    "            pixel_values_np = inputs['pixel_values'].cpu().numpy()\n",
    "            \n",
    "            print(f\"NumPy pixel_values shape: {pixel_values_np.shape}\")\n",
    "            print(f\"pixel_values dtype: {pixel_values_np.dtype}\")\n",
    "\n",
    "            # 3. Run image-only inference using the TensorRT wrapper\n",
    "            try:\n",
    "                # The TensorRT engine outputs image features\n",
    "                image_features_np = trt_model.infer(pixel_values=pixel_values_np)\n",
    "                print(f\"TensorRT output (image_features) shape: {image_features_np.shape}\")\n",
    "                \n",
    "                # 4. Convert the NumPy output back to a Torch tensor for post-processing\n",
    "                image_features = torch.from_numpy(image_features_np).to(device)\n",
    "                # Normalize image features as done in CLIP\n",
    "                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True) \n",
    "\n",
    "                # 5. Calculate logits (similarity scores) by performing dot product\n",
    "                #    between image features and the pre-encoded text features.\n",
    "                #    This is how CLIP computes similarity.\n",
    "                logits = torch.matmul(image_features, text_features.T) # .T transposes text_features\n",
    "                \n",
    "                # Get probabilities and predictions for the current batch\n",
    "                \n",
    "                probs = logits.sigmoid() # For multi-label classification\n",
    "                predictions = (probs > CONFIDENCE_THRESHOLD).int()\n",
    "\n",
    "                all_probs.append(probs.cpu())\n",
    "                image_patch_predictions.append(predictions.cpu())\n",
    "                \n",
    "                print(f\"Batch processed successfully!\")\n",
    "                print(f\"Probabilities shape: {probs.shape}\")\n",
    "                print(f\"Predictions shape: {predictions.shape}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error during TensorRT inference: {e}\")\n",
    "                print(f\"Error type: {type(e)}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break # Exit loop on error\n",
    "\n",
    "    if all_probs:  # Only continue if we have results\n",
    "        # Concatenate results from all batches\n",
    "        full_probs_tensor = torch.cat(all_probs)\n",
    "        full_predictions_tensor = torch.cat(image_patch_predictions)\n",
    "        \n",
    "        print(f\"\\nFinal concatenated results:\")\n",
    "        print(f\"full_probs_tensor shape: {full_probs_tensor.shape}\")\n",
    "        print(f\"full_predictions_tensor shape: {full_predictions_tensor.shape}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"--> Image processing finished in {end_time - start_time:.2f} seconds.\")\n",
    "    \n",
    "    break  # Only process first image for debugging\n",
    "\n",
    "print(\"\\n\\n--- Debug analysis complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e500c37-8451-48d7-9075-d12aa70e7c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running OPTIMIZED Sliding Window Analysis on All 60 Images ---\n",
      "\n",
      "\n",
      "=========================================================\n",
      "Processing Image: IMG_2197_JPEG.rf.912aa4bfc6ecb5926978c385be331efe.jpg\n",
      "=============================================================\n",
      "Extracted 910 patches. Processing them in batches of 8...\n",
      "Grid dimensions: 35 x 26\n",
      "Batch 1 processed successfully!\n",
      "Batch 2 processed successfully!\n",
      "Batch 3 processed successfully!\n",
      "Batch 4 processed successfully!\n",
      "Batch 5 processed successfully!\n",
      "Batch 6 processed successfully!\n",
      "Batch 7 processed successfully!\n",
      "Batch 8 processed successfully!\n",
      "Batch 9 processed successfully!\n",
      "Batch 10 processed successfully!\n",
      "Batch 11 processed successfully!\n",
      "Batch 12 processed successfully!\n",
      "Batch 13 processed successfully!\n",
      "Batch 14 processed successfully!\n",
      "Batch 15 processed successfully!\n",
      "Batch 16 processed successfully!\n",
      "Batch 17 processed successfully!\n",
      "Batch 18 processed successfully!\n",
      "Batch 19 processed successfully!\n",
      "Batch 20 processed successfully!\n",
      "Batch 21 processed successfully!\n",
      "Batch 22 processed successfully!\n",
      "Batch 23 processed successfully!\n",
      "Batch 24 processed successfully!\n",
      "Batch 25 processed successfully!\n",
      "Batch 26 processed successfully!\n",
      "Batch 27 processed successfully!\n",
      "Batch 28 processed successfully!\n",
      "Batch 29 processed successfully!\n",
      "Batch 30 processed successfully!\n",
      "Batch 31 processed successfully!\n",
      "Batch 32 processed successfully!\n",
      "Batch 33 processed successfully!\n",
      "Batch 34 processed successfully!\n",
      "Batch 35 processed successfully!\n",
      "Batch 36 processed successfully!\n",
      "Batch 37 processed successfully!\n",
      "Batch 38 processed successfully!\n",
      "Batch 39 processed successfully!\n",
      "Batch 40 processed successfully!\n",
      "Batch 41 processed successfully!\n",
      "Batch 42 processed successfully!\n",
      "Batch 43 processed successfully!\n",
      "Batch 44 processed successfully!\n",
      "Batch 45 processed successfully!\n",
      "Batch 46 processed successfully!\n",
      "Batch 47 processed successfully!\n",
      "Batch 48 processed successfully!\n",
      "Batch 49 processed successfully!\n",
      "Batch 50 processed successfully!\n",
      "Batch 51 processed successfully!\n",
      "Batch 52 processed successfully!\n",
      "Batch 53 processed successfully!\n",
      "Batch 54 processed successfully!\n",
      "Batch 55 processed successfully!\n",
      "Batch 56 processed successfully!\n",
      "Batch 57 processed successfully!\n",
      "Batch 58 processed successfully!\n",
      "Batch 59 processed successfully!\n",
      "Batch 60 processed successfully!\n",
      "Batch 61 processed successfully!\n",
      "Batch 62 processed successfully!\n",
      "Batch 63 processed successfully!\n",
      "Batch 64 processed successfully!\n",
      "Batch 65 processed successfully!\n",
      "Batch 66 processed successfully!\n",
      "Batch 67 processed successfully!\n",
      "Batch 68 processed successfully!\n",
      "Batch 69 processed successfully!\n",
      "Batch 70 processed successfully!\n",
      "Batch 71 processed successfully!\n",
      "Batch 72 processed successfully!\n",
      "Batch 73 processed successfully!\n",
      "Batch 74 processed successfully!\n",
      "Batch 75 processed successfully!\n",
      "Batch 76 processed successfully!\n",
      "Batch 77 processed successfully!\n",
      "Batch 78 processed successfully!\n",
      "Batch 79 processed successfully!\n",
      "Batch 80 processed successfully!\n",
      "Batch 81 processed successfully!\n",
      "Batch 82 processed successfully!\n",
      "Batch 83 processed successfully!\n",
      "Batch 84 processed successfully!\n",
      "Batch 85 processed successfully!\n",
      "Batch 86 processed successfully!\n",
      "Batch 87 processed successfully!\n",
      "Batch 88 processed successfully!\n",
      "Batch 89 processed successfully!\n",
      "Batch 90 processed successfully!\n",
      "Batch 91 processed successfully!\n",
      "Batch 92 processed successfully!\n",
      "Batch 93 processed successfully!\n",
      "Batch 94 processed successfully!\n",
      "Batch 95 processed successfully!\n",
      "Batch 96 processed successfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 3. Run image-only inference using the TensorRT wrapper\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# The TensorRT engine outputs image features\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     image_features_np \u001b[38;5;241m=\u001b[39m \u001b[43mtrt_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m# 4. Convert the NumPy output back to a Torch tensor for post-processing\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     image_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(image_features_np)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[25], line 86\u001b[0m, in \u001b[0;36mTensorRTInference.infer\u001b[0;34m(self, pixel_values)\u001b[0m\n\u001b[1;32m     83\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_htod_async(d_input, pixel_values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Execute inference\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_async_v3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_handle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorRT inference execution failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Allocate host memory for output and copy back\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- 5. OPTIMIZED SLIDING WINDOW INFERENCE LOOP WITH HEATMAP GENERATION ---\n",
    "print(f\"--- Running OPTIMIZED Sliding Window Analysis on All {len(image_id_to_filename_subset)} Images ---\")\n",
    "\n",
    "# List to store processing times for each image\n",
    "image_processing_times = [] # New: Initialize list to store times\n",
    "\n",
    "for image_id, filename in list(image_id_to_filename_subset.items())[:15]:\n",
    "    print(f\"\\n\\n=========================================================\")\n",
    "    print(f\"Processing Image: {filename}\")\n",
    "    print(f\"=============================================================\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    image_path = os.path.join(VALIDATION_DIR, filename)\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"--> SKIPPING: File not found at {image_path}\")\n",
    "        continue\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_width, image_height = image.size\n",
    "    current_image_annotations = image_id_to_annotations[image_id]\n",
    "\n",
    "    # --- Extract all patches into a list first ---\n",
    "    patches = []\n",
    "    patch_coordinates = []  # Store coordinates for ground truth calculation if needed\n",
    "    num_patches_y = (image_height - PATCH_SIZE) // STRIDE + 1\n",
    "    num_patches_x = (image_width - PATCH_SIZE) // STRIDE + 1\n",
    "\n",
    "    for y in range(0, image_height - PATCH_SIZE + 1, STRIDE):\n",
    "        for x in range(0, image_width - PATCH_SIZE + 1, STRIDE):\n",
    "            patch = image.crop((x, y, x + PATCH_SIZE, y + PATCH_SIZE))\n",
    "            patches.append(patch)\n",
    "            patch_coordinates.append((x, y, x + PATCH_SIZE, y + PATCH_SIZE))\n",
    "\n",
    "    if not patches:\n",
    "        print(\"--> SKIPPING: No patches were generated for this image.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Extracted {len(patches)} patches. Processing them in batches of {BATCH_SIZE}...\")\n",
    "    print(f\"Grid dimensions: {num_patches_y} x {num_patches_x}\")\n",
    "\n",
    "    all_probs = []\n",
    "    image_patch_predictions = []\n",
    "\n",
    "    # Process all patches in batches (not just the first batch)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(patches), BATCH_SIZE):\n",
    "            batch = patches[i:i + BATCH_SIZE]\n",
    "            batch_num = i // BATCH_SIZE + 1\n",
    "            #print(f\"--- Processing batch {batch_num}/{(len(patches) + BATCH_SIZE - 1) // BATCH_SIZE} with {len(batch)} patches ---\")\n",
    "\n",
    "            # 1. Preprocess images with Hugging Face processor\n",
    "            inputs = processor(images=batch, return_tensors=\"pt\").to(device)\n",
    "\n",
    "            # 2. Convert PyTorch pixel_values tensor to NumPy array for the TRT engine\n",
    "            pixel_values_np = inputs['pixel_values'].cpu().numpy()\n",
    "\n",
    "            # 3. Run image-only inference using the TensorRT wrapper\n",
    "            try:\n",
    "                # The TensorRT engine outputs image features\n",
    "                image_features_np = trt_model.infer(pixel_values=pixel_values_np)\n",
    "\n",
    "                # 4. Convert the NumPy output back to a Torch tensor for post-processing\n",
    "                image_features = torch.from_numpy(image_features_np).to(device)\n",
    "                # Normalize image features as done in CLIP\n",
    "                image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "\n",
    "                # 5. Calculate logits (similarity scores) by performing dot product\n",
    "                #    between image features and the pre-encoded text features.\n",
    "                logits = torch.matmul(image_features, text_features.T)\n",
    "\n",
    "                # Get probabilities and predictions for the current batch\n",
    "                #print(f\"Logits min: {logits.min()}, max: {logits.max()}, mean: {logits.mean()}, std: {logits.std()}\")\n",
    "                probs = logits.sigmoid()  # For multi-label classification\n",
    "                predictions = (probs > CONFIDENCE_THRESHOLD).int()\n",
    "\n",
    "                all_probs.append(probs.cpu())\n",
    "                image_patch_predictions.append(predictions.cpu())\n",
    "\n",
    "                print(f\"Batch {batch_num} processed successfully!\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during TensorRT inference in batch {batch_num}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                break  # Exit loop on error\n",
    "\n",
    "    if not all_probs:  # If no batches were processed successfully\n",
    "        print(\"--> SKIPPING: No patches were processed successfully.\")\n",
    "        continue\n",
    "\n",
    "    # Concatenate results from all batches\n",
    "    full_probs_tensor = torch.cat(all_probs)\n",
    "    full_predictions_tensor = torch.cat(image_patch_predictions)\n",
    "\n",
    "    print(f\"\\nFinal results:\")\n",
    "    print(f\"full_probs_tensor shape: {full_probs_tensor.shape}\")\n",
    "    print(f\"full_predictions_tensor shape: {full_predictions_tensor.shape}\")\n",
    "    print(f\"Expected patches: {num_patches_y * num_patches_x}\")\n",
    "\n",
    "    # --- HEATMAP GENERATION ---\n",
    "    # Reshape the results to form the heatmap\n",
    "    # The output order is preserved, so we can directly reshape.\n",
    "    # Reshape from (total_patches, num_classes) to (y_dim, x_dim, num_classes)\n",
    "    heatmap_tensor = full_probs_tensor.view(num_patches_y, num_patches_x, len(CLASS_LABELS))\n",
    "    # Permute to get (num_classes, y_dim, x_dim) for easy plotting\n",
    "    heatmap = heatmap_tensor.permute(2, 0, 1)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    image_processing_times.append(elapsed_time) # New: Store elapsed time\n",
    "    print(f\"--> Image processing finished in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # --- Display Per-Image Report ---\n",
    "    #print(f\"\\nImage Report Card for: {filename}\")\n",
    "    #print(\"-\" * 60)\n",
    "    #print(f\"{'Class':<12} | {'Ground Truth Count':<20} | {'Predicted Patch Count':<22}\")\n",
    "    #print(\"-\" * 60)\n",
    "\n",
    "    # Count ground truth annotations\n",
    "    true_counts = [0] * len(train_class_labels)\n",
    "    for ann in current_image_annotations:\n",
    "        class_idx = coco_id_to_class_idx.get(ann['category_id'])\n",
    "        if class_idx is not None:\n",
    "            true_counts[class_idx] += 1\n",
    "\n",
    "    # Sum the predictions from the batched tensor result\n",
    "    predicted_counts = torch.sum(full_predictions_tensor[:, :len(train_class_labels)], axis=0).numpy()\n",
    "    for i, label in enumerate(train_class_labels):\n",
    "        print(f\"{label:<12} | {true_counts[i]:<20} | {predicted_counts[i]:<22}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # --- Display Per-Image Heatmaps ---\n",
    "    print(f\"\\nHeatmap Visualizations for: {filename}\")\n",
    "    heatmap_np = heatmap.numpy()\n",
    "    fig, axes = plt.subplots(1, len(CLASS_LABELS) + 1, figsize=(20, 5))\n",
    "\n",
    "    # Show original image\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    custom_vmin = 0.45\n",
    "    custom_vmax = 0.55\n",
    "\n",
    "    # Show heatmaps for each class\n",
    "    for i, class_name in enumerate(CLASS_LABELS):\n",
    "        im = axes[i+1].imshow(heatmap_np[i], cmap='viridis', interpolation='nearest',\n",
    "                               vmin=custom_vmin, vmax=custom_vmax)\n",
    "        axes[i+1].set_title(f\"Heatmap for '{class_name}'\")\n",
    "        axes[i+1].axis('off')\n",
    "\n",
    "    # Add colorbar\n",
    "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Optional: Break after first image for testing\n",
    "    # Remove this break to process all images\n",
    "    #break # Keep this commented to process all 15 images\n",
    "\n",
    "# New: Calculate and print the mean processing time\n",
    "if image_processing_times:\n",
    "    mean_time = sum(image_processing_times) / len(image_processing_times)\n",
    "    print(f\"\\n\\n--- Mean time to process a full image: {mean_time:.2f} seconds ---\")\n",
    "else:\n",
    "    print(\"\\n\\n--- No images were processed to calculate mean time. ---\")\n",
    "\n",
    "print(\"\\n\\n--- Analysis of validation images complete. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5568483-a09c-4b38-9fcf-b46aac06df2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
