{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhrnVAeKrJLw"
      },
      "source": [
        "# FINE TUNING TINYCLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "bNy-koHDrJgB"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch torch torchvision pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQpoIrT2xGcQ"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dh-vvPTLYH9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "!unzip /content/crops.zip -d /content/train/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh_OMY17vPyB"
      },
      "source": [
        "## CREATE CSV DATA FILE: IMG_PATH, CAPTION TEXT PROMPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMjYHGfetPl2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# The main directory where your class folders are located\n",
        "CROPS_DIR = '/content/train/crops'\n",
        "# The final CSV file to be created\n",
        "OUTPUT_CSV_PATH = 'training_data.csv'\n",
        "# The class labels, in the desired order for the vector\n",
        "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
        "\n",
        "# --- SCRIPT LOGIC ---\n",
        "print(f\"--- Generating '{OUTPUT_CSV_PATH}' from '{CROPS_DIR}' ---\")\n",
        "\n",
        "# A list to hold all the data rows\n",
        "training_data = []\n",
        "\n",
        "# Walk through the directory\n",
        "for class_name in os.listdir(CROPS_DIR):\n",
        "    class_dir = os.path.join(CROPS_DIR, class_name)\n",
        "    if not os.path.isdir(class_dir) or class_name not in CLASS_LABELS:\n",
        "        continue\n",
        "\n",
        "    print(f\"Processing folder: {class_name}\")\n",
        "\n",
        "    # Get the index for the current class to create the one-hot vector\n",
        "    true_class_index = CLASS_LABELS.index(class_name)\n",
        "\n",
        "    for filename in os.listdir(class_dir):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "            image_path = os.path.join(class_dir, filename)\n",
        "\n",
        "            # Create the multi-hot (in this case, one-hot) label vector\n",
        "            label_vector = [0] * len(CLASS_LABELS)\n",
        "            label_vector[true_class_index] = 1\n",
        "\n",
        "            # Add data to our list\n",
        "            row = {'image_path': image_path}\n",
        "            row.update({label: vec_val for label, vec_val in zip(CLASS_LABELS, label_vector)})\n",
        "            training_data.append(row)\n",
        "\n",
        "# Convert the list of data to a pandas DataFrame\n",
        "df = pd.DataFrame(training_data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
        "\n",
        "print(f\"\\nSuccessfully created '{OUTPUT_CSV_PATH}' with {len(df)} entries. âœ…\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQjvD2fvc3U"
      },
      "source": [
        "## TRAINING\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3bojxuSt49T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## 1. Configuration\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-5\n",
        "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "CSV_PATH = 'training_data.csv'  # The CSV you just created\n",
        "SAVE_PATH = '/content/finetuned_tinyclip_multilabel.pt'\n",
        "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
        "\n",
        "## 2. Custom Dataset\n",
        "class MultiLabelDataset(Dataset):\n",
        "    def __init__(self, csv_path, class_labels):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.class_labels = class_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = row['image_path']\n",
        "        labels = torch.tensor([row[label] for label in self.class_labels], dtype=torch.float32)\n",
        "        return {\"image_path\": image_path, \"labels\": labels}\n",
        "\n",
        "## 3. Main Training Logic\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "print(\"Model loaded successfully. âœ…\")\n",
        "\n",
        "dataset = MultiLabelDataset(CSV_PATH, CLASS_LABELS)\n",
        "text_prompts = [f\"a photo of a {label}\" for label in CLASS_LABELS]\n",
        "text_inputs = processor(text=text_prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "with torch.no_grad():\n",
        "    text_embeds = model.get_text_features(**text_inputs)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    image_paths = [item['image_path'] for item in batch]\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    try:\n",
        "        images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading image: {e}\")\n",
        "        return None\n",
        "    image_inputs = processor(images=images, return_tensors=\"pt\", padding=True)\n",
        "    return {\"pixel_values\": image_inputs.pixel_values, \"labels\": labels}\n",
        "\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "print(\"Starting multi-label fine-tuning...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    for batch in pbar:\n",
        "        if batch is None: continue\n",
        "\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        image_embeds = model.get_image_features(pixel_values=pixel_values)\n",
        "        logits = torch.matmul(image_embeds, text_embeds.t())\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch {epoch + 1} finished with average loss: {avg_loss:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), SAVE_PATH)\n",
        "print(f\"\\nTraining complete! ðŸŽ‰ Multi-label model saved to {SAVE_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gODzzMgCBjcX"
      },
      "source": [
        "## CREATE VALIDATION DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdHhBmUGwR_D"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95ZYkmUuw2Ai"
      },
      "outputs": [],
      "source": [
        "!unzip /content/valid.zip -d /content/validation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSNaY3Fkx729"
      },
      "source": [
        "## **EVALUATION SCRIPT**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqja7jiKw4ig"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "## 1. Configuration\n",
        "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "SAVED_MODEL_PATH = '/content/finetuned_tinyclip_multilabel.pt'\n",
        "VALIDATION_DIR = '/content/validation/valid/crops'\n",
        "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
        "\n",
        "\n",
        "## 2. Load Model and Processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model = CLIPModel.from_pretrained(MODEL_ID)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "\n",
        "## 3. Evaluation\n",
        "text_prompts = [f\"a photo of a {label}\" for label in CLASS_LABELS]\n",
        "print(f\"Testing against prompts: {text_prompts}\")\n",
        "\n",
        "all_true_labels = []\n",
        "all_predicted_labels = []\n",
        "all_pred_scores = []\n",
        "\n",
        "if not os.path.isdir(VALIDATION_DIR):\n",
        "    print(f\"Error: Validation directory '{VALIDATION_DIR}' not found.\")\n",
        "else:\n",
        "    pbar = tqdm(os.walk(VALIDATION_DIR), desc=\"Evaluating\")\n",
        "    for root, _, files in pbar:\n",
        "        for file in files:\n",
        "            if not file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            image_path = os.path.join(root, file)\n",
        "            true_label = os.path.basename(root)\n",
        "\n",
        "            image = Image.open(image_path).convert(\"RGB\")\n",
        "            inputs = processor(\n",
        "                text=text_prompts,\n",
        "                images=image,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            ).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            logits_per_image = outputs.logits_per_image\n",
        "            predicted_index = logits_per_image.argmax().item()\n",
        "            predicted_label = CLASS_LABELS[predicted_index]\n",
        "\n",
        "            all_true_labels.append(true_label)\n",
        "            all_predicted_labels.append(predicted_label)\n",
        "            probs = logits_per_image.softmax(dim=1).cpu().numpy()\n",
        "            all_pred_scores.append(probs[0])\n",
        "\n",
        "            pbar.set_postfix({\"Correct\": f\"{len([i for i, j in zip(all_predicted_labels, all_true_labels) if i == j])}/{len(all_true_labels)}\"})\n",
        "\n",
        "    if all_true_labels:\n",
        "        print(\"\\n--- Evaluation Complete ---\")\n",
        "        report = classification_report(all_true_labels, all_predicted_labels, labels=CLASS_LABELS, digits=4)\n",
        "        print(\"--- Classification Report ---\")\n",
        "        print(report)\n",
        "\n",
        "        print(\"\\n--- Confusion Matrix ---\")\n",
        "        cm = confusion_matrix(all_true_labels, all_predicted_labels, labels=CLASS_LABELS)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=CLASS_LABELS, yticklabels=CLASS_LABELS)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "        # --- Added Normalized Confusion Matrix ---\n",
        "        print(\"\\n--- Normalized Confusion Matrix ---\")\n",
        "        cm_normalized = confusion_matrix(all_true_labels, all_predicted_labels, labels=CLASS_LABELS, normalize='true')\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
        "                    xticklabels=CLASS_LABELS, yticklabels=CLASS_LABELS)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Normalized Confusion Matrix')\n",
        "        plt.show()\n",
        "\n",
        "        # Binarize labels and convert scores for plotting\n",
        "        lb = LabelBinarizer()\n",
        "        lb.fit(CLASS_LABELS)\n",
        "        y_true_bin = lb.transform(all_true_labels)\n",
        "        y_scores = np.array(all_pred_scores)\n",
        "\n",
        "        # --- FIGURE 1: All P-R curves on one plot with an inset ---\n",
        "        print(\"\\n--- Combined Precision-Recall Curves ---\")\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        for i, class_name in enumerate(CLASS_LABELS):\n",
        "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "            # --- MODIFIED: Linewidth is thicker only for 'calyx' ---\n",
        "            linewidth = 4 if class_name == 'calyx' else 2\n",
        "            ax.plot(recall, precision, lw=linewidth, label=f'P-R curve for {class_name}')\n",
        "\n",
        "        ax.set_xlabel(\"Recall\")\n",
        "        ax.set_ylabel(\"Precision\")\n",
        "        ax.set_title(\"Precision-Recall Curves for the TinyCLIP Classifier\")\n",
        "        ax.legend(loc=\"best\")\n",
        "        ax.grid(alpha=0.4)\n",
        "        ax.set_xlim(0.75, 1.05)\n",
        "        ax.set_ylim(0.75, 1.05)\n",
        "\n",
        "        # --- ADD INSET PLOT FOR CONTEXT ---\n",
        "        ax_inset = fig.add_axes([0.18, 0.18, 0.4, 0.4])\n",
        "        for i, class_name in enumerate(CLASS_LABELS):\n",
        "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "            # --- MODIFIED: Inset linewidth is also thicker only for 'calyx' ---\n",
        "            linewidth = 4 if class_name == 'calyx' else 2\n",
        "            ax_inset.plot(recall, precision, lw=linewidth)\n",
        "        ax_inset.set_title(\"Full Range (0-1)\")\n",
        "        ax_inset.set_xlabel(\"Recall\")\n",
        "        ax_inset.set_ylabel(\"Precision\")\n",
        "        ax_inset.grid(alpha=0.4)\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # --- FIGURE 2: P-R curve for each class as a separate subplot with insets ---\n",
        "        print(\"\\n--- Per-Class Precision-Recall Curves ---\")\n",
        "        fig, axes = plt.subplots(1, len(CLASS_LABELS), figsize=(20, 6), sharey=True)\n",
        "        fig.suptitle('Per-Class Precision-Recall Curves', fontsize=16)\n",
        "\n",
        "        for i, class_name in enumerate(CLASS_LABELS):\n",
        "            precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_scores[:, i])\n",
        "            # --- REVERTED: All lines have the same standard thickness ---\n",
        "            axes[i].plot(recall, precision, lw=2, label=f'P-R for {class_name}')\n",
        "            axes[i].set_title(f'Class: {class_name}')\n",
        "            axes[i].set_xlabel('Recall')\n",
        "            axes[i].set_ylabel('Precision')\n",
        "            axes[i].grid(alpha=0.4)\n",
        "            axes[i].legend()\n",
        "            axes[i].set_xlim(0.75, 1.05)\n",
        "            axes[i].set_ylim(0.75, 1.05)\n",
        "\n",
        "            # --- ADD INSET PLOT FOR EACH SUBPLOT ---\n",
        "            ax_inset = axes[i].inset_axes([0.1, 0.1, 0.5, 0.5])\n",
        "            ax_inset.plot(recall, precision, lw=2)\n",
        "            ax_inset.set_title(\"Full Range\")\n",
        "            ax_inset.set_xlabel(\"R\", fontsize=8)\n",
        "            ax_inset.set_ylabel(\"P\", fontsize=8)\n",
        "            ax_inset.grid(alpha=0.4)\n",
        "\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "    else:\n",
        "        print(\"No images found in the validation directory.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-vSQGZ-ecGA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "## 1. Configuration\n",
        "SAVED_MODEL_PATH = '/content/finetuned_tinyclip_multilabel.pt'\n",
        "VALIDATION_DIR = '/content/validation/valid/crops'\n",
        "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "\n",
        "# --- Check if GPU is available ---\n",
        "if not torch.cuda.is_available():\n",
        "    print(\"Error: A CUDA-enabled GPU is required for accurate measurements.\")\n",
        "else:\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    ## 2. Load Model and Data Paths\n",
        "    model = CLIPModel.from_pretrained(MODEL_ID)\n",
        "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "\n",
        "    # Get list of image paths for evaluation\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(VALIDATION_DIR):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    num_images = len(image_paths)\n",
        "    print(f\"Found {num_images} images for evaluation.\")\n",
        "\n",
        "    # --- Warm-up the GPU ---\n",
        "    print(\"Warming up the GPU...\")\n",
        "    for _ in range(10):\n",
        "        dummy_image = Image.new('RGB', (224, 224))\n",
        "        dummy_text = [\"a photo\"]\n",
        "        inputs = processor(text=dummy_text, images=dummy_image, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # ===================================================================\n",
        "    # METRIC 1: MODEL SIZE\n",
        "    # ===================================================================\n",
        "    model_size_bytes = os.path.getsize(SAVED_MODEL_PATH)\n",
        "    model_size_mb = model_size_bytes / (1024 * 1024)\n",
        "    print(f\"\\n--- 1. Model Size ---\")\n",
        "    print(f\"âœ… Model Size: {model_size_mb:.2f} MB\")\n",
        "\n",
        "    # ===================================================================\n",
        "    # METRIC 2: INFERENCE LATENCY\n",
        "    # ===================================================================\n",
        "    print(\"\\n--- 2. Measuring Inference Latency ---\")\n",
        "    latencies = []\n",
        "    for image_path in tqdm(image_paths, desc=\"Measuring Latency\"):\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        text_prompts = [\"a photo\"] # Only need one prompt for timing\n",
        "        inputs = processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        # Time the inference call\n",
        "        start_event = torch.cuda.Event(enable_timing=True)\n",
        "        end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "        start_event.record()\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "        end_event.record()\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        latency_ms = start_event.elapsed_time(end_event)\n",
        "        latencies.append(latency_ms)\n",
        "\n",
        "    average_latency_ms = np.mean(latencies)\n",
        "    print(f\"âœ… Average Latency: {average_latency_ms:.2f} ms per image\")\n",
        "\n",
        "\n",
        "    # ===================================================================\n",
        "    # METRIC 3: PEAK MEMORY USAGE\n",
        "    # ===================================================================\n",
        "    print(\"\\n--- 3. Measuring Peak Memory Usage ---\")\n",
        "    torch.cuda.reset_peak_memory_stats(device)\n",
        "\n",
        "    # Run evaluation on all images to find the peak memory\n",
        "    for image_path in tqdm(image_paths, desc=\"Measuring Memory\"):\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        text_prompts = [\"a photo\"]\n",
        "        inputs = processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            _ = model(**inputs)\n",
        "\n",
        "    peak_memory_bytes = torch.cuda.max_memory_allocated(device)\n",
        "    peak_memory_mb = peak_memory_bytes / (1024 * 1024)\n",
        "    print(f\"âœ… Peak Memory Usage: {peak_memory_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgSTH0CILCtz"
      },
      "source": [
        "## MISCLASSIFICATION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import os\n",
        "import torch\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "from PIL import Image\n",
        "\n",
        "# --- 1. Configuration ---\n",
        "# Model and Data Paths\n",
        "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "SAVED_MODEL_PATH = '/content/finetuned_tinyclip_multilabel.pt'\n",
        "VALIDATION_CSV_PATH = 'validation_data.csv'\n",
        "\n",
        "# Image and Class Configuration\n",
        "VALIDATION_ROOT_DIR = '/content/validation/valid'\n",
        "ORIGINAL_IMAGE_BASE_FILENAME = 'IMG_2021_JPEG.rf.d08beab9aa2e94e562953893e89bdc15.jpg'\n",
        "IMAGE_IDENTIFIER = 'IMG_2021' # The common identifier for the image and its crops\n",
        "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
        "\n",
        "# --- 2. Load Model and Data ---\n",
        "# Load fine-tuned model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "try:\n",
        "    model = CLIPModel.from_pretrained(MODEL_ID)\n",
        "    model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "    text_prompts = [f\"a photo of a {label}\" for label in CLASS_LABELS]\n",
        "    print(\"Successfully loaded model and processor.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Load the dataset CSV\n",
        "try:\n",
        "    df = pd.read_csv(VALIDATION_CSV_PATH)\n",
        "    print(f\"Successfully loaded data from '{VALIDATION_CSV_PATH}'.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Validation CSV '{VALIDATION_CSV_PATH}' not found.\")\n",
        "    exit()\n",
        "\n",
        "# --- 3. Identify Target Images ---\n",
        "# Path to the original, un-cropped image\n",
        "original_image_path = os.path.join(VALIDATION_ROOT_DIR, ORIGINAL_IMAGE_BASE_FILENAME)\n",
        "\n",
        "# Find all rows in the CSV related to the target image (including positive and negative crops)\n",
        "cropped_images_info = df[df['image_path'].str.contains(IMAGE_IDENTIFIER, na=False)]\n",
        "cropped_image_paths = cropped_images_info['image_path'].tolist()\n",
        "\n",
        "if not cropped_image_paths:\n",
        "    print(f\"No cropped images found for identifier '{IMAGE_IDENTIFIER}'. Check your CSV and identifier.\")\n",
        "    exit()\n",
        "\n",
        "# --- 4. Create Display with Predictions ---\n",
        "# Arrange plots in a grid\n",
        "num_cropped = len(cropped_image_paths)\n",
        "total_plots = 1 + num_cropped\n",
        "num_cols = 3\n",
        "num_rows = (total_plots + num_cols - 1) // num_cols\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 6))\n",
        "axes = axes.flatten()\n",
        "for ax in axes:\n",
        "    ax.axis('off')\n",
        "\n",
        "# --- Plot the Original Image (First Slot) ---\n",
        "ax_idx = 0\n",
        "axes[ax_idx].set_title(f\"Original:\\n{os.path.basename(original_image_path)}\", fontsize=10)\n",
        "if os.path.exists(original_image_path):\n",
        "    try:\n",
        "        img = mpimg.imread(original_image_path)\n",
        "        axes[ax_idx].imshow(img)\n",
        "    except Exception as e:\n",
        "        axes[ax_idx].text(0.5, 0.5, f'Error loading\\n{e}', ha='center', color='red')\n",
        "else:\n",
        "    axes[ax_idx].text(0.5, 0.5, 'Original Image Not Found', ha='center', color='red')\n",
        "\n",
        "# --- Plot the Cropped Images with Predictions ---\n",
        "for i, path in enumerate(cropped_image_paths):\n",
        "    ax_idx = i + 1\n",
        "    if ax_idx >= len(axes):\n",
        "        break # Stop if we run out of subplot axes\n",
        "\n",
        "    ax = axes[ax_idx]\n",
        "    if not os.path.exists(path):\n",
        "        ax.text(0.5, 0.5, 'Image File Not Found', ha='center', color='red')\n",
        "        ax.set_title(f\"Crop:\\n{os.path.basename(path)}\", fontsize=10)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Get the ground truth label from the DataFrame\n",
        "        image_data = cropped_images_info[cropped_images_info['image_path'] == path].iloc[0]\n",
        "        true_label = 'unknown'\n",
        "        for label in CLASS_LABELS:\n",
        "            if image_data[label] == 1:\n",
        "                true_label = label\n",
        "                break\n",
        "\n",
        "        # Get the model's prediction\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        inputs = processor(text=text_prompts, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "        logits = outputs.logits_per_image\n",
        "        pred_index = logits.argmax().item()\n",
        "        predicted_label = CLASS_LABELS[pred_index]\n",
        "\n",
        "        # Display the image\n",
        "        ax.imshow(mpimg.imread(path))\n",
        "\n",
        "        # Set the title with the prediction results and appropriate color\n",
        "        if predicted_label == true_label:\n",
        "            title_color = 'green'\n",
        "        else:\n",
        "            title_color = 'red'\n",
        "\n",
        "        ax.set_title(f\"True: '{true_label}'\\nPred: '{predicted_label}'\", color=title_color, fontsize=12)\n",
        "\n",
        "    except Exception as e:\n",
        "        ax.text(0.5, 0.5, f'Error processing\\n{e}', ha='center', color='red')\n",
        "        print(f\"Failed to process {path}: {e}\")\n",
        "\n",
        "# Main title for the entire figure\n",
        "plt.suptitle(f'Model Predictions for {IMAGE_IDENTIFIER}', fontsize=20, y=1.0)\n",
        "plt.tight_layout(pad=3.0)\n",
        "plt.savefig(IMAGE_IDENTIFIER + \".png\")\n",
        "# plt.show() # Uncomment to display in an interactive environment\n",
        "\n",
        "print(f\"\\nGenerated plot and saved to {IMAGE_IDENTIFIER}.png\")"
      ],
      "metadata": {
        "id": "EZp-GmLaUgaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHFjPfUCUBBo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kUQjvD2fvc3U"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}