{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGLfJgHarqep",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch torch torchvision pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/validation\n",
        "!unzip /content/valid.zip -d /content/validation/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-ZnlA9t7uzFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HEATMAPS\n"
      ],
      "metadata": {
        "id": "ze4vnhKvA6Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "import json\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time # Import time to measure performance\n",
        "\n",
        "# Suppress specific warnings from PIL about large images\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='PIL')\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MODEL_ID = \"wkcn/TinyCLIP-ViT-61M-32-Text-29M-LAION400M\"\n",
        "SAVED_MODEL_PATH = '/content/finetuned_tinyclip_multilabel.pt'\n",
        "VALIDATION_DIR = '/content/validation/valid'\n",
        "CLASS_LABELS = [\"calyx\", \"fruitlet\", \"peduncle\", \"negative\"]\n",
        "CONFIDENCE_THRESHOLD = 0.5\n",
        "\n",
        "# --- NEW: BATCH_SIZE for processing patches to prevent out-of-memory errors ---\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Sliding Window Parameters\n",
        "PATCH_SIZE = 224\n",
        "STRIDE = 112\n",
        "\n",
        "# --- 2. MODEL AND PROCESSOR SETUP ---\n",
        "print(\"--- Loading FINE-TUNED Model for Stage 2 ---\")\n",
        "if not os.path.exists(SAVED_MODEL_PATH):\n",
        "    raise FileNotFoundError(f\"Fine-tuned model not found at {SAVED_MODEL_PATH}.\")\n",
        "\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
        "model = CLIPModel.from_pretrained(MODEL_ID)\n",
        "model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=torch.device('cpu')))\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()\n",
        "text_prompts = [f\"a photo of a {label}\" for label in CLASS_LABELS]\n",
        "print(f\"Using device: {device}\\n\")\n",
        "\n",
        "\n",
        "# --- 3. LOAD AND PROCESS GROUND TRUTH ---\n",
        "print(\"--- Loading and Processing COCO Ground Truth ---\")\n",
        "annotation_file_path = os.path.join(VALIDATION_DIR, '_annotations.coco.json')\n",
        "if not os.path.exists(annotation_file_path):\n",
        "    raise FileNotFoundError(f\"Annotation file not found at {annotation_file_path}.\")\n",
        "\n",
        "with open(annotation_file_path, 'r') as f:\n",
        "    coco_data = json.load(f)\n",
        "\n",
        "# Create mappings and load all annotations\n",
        "coco_id_to_name = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
        "train_class_labels = [lbl for lbl in CLASS_LABELS if lbl != \"negative\"]\n",
        "name_to_class_idx = {name: i for i, name in enumerate(train_class_labels)}\n",
        "coco_id_to_class_idx = {\n",
        "    coco_id: name_to_class_idx.get(name) for coco_id, name in coco_id_to_name.items() if name in train_class_labels\n",
        "}\n",
        "\n",
        "image_id_to_filename = {img['id']: img['file_name'] for img in coco_data['images']}\n",
        "image_id_to_filename_subset = dict(list(image_id_to_filename.items())[:])\n",
        "image_id_to_annotations = {img_id: [] for img_id in image_id_to_filename}\n",
        "for ann in coco_data['annotations']:\n",
        "    image_id_to_annotations[ann['image_id']].append(ann)\n",
        "print(f\"Processed ground truth for {len(image_id_to_filename)} images.\\n\")\n",
        "\n",
        "\n",
        "# --- 4. HELPER FUNCTION (Unchanged) ---\n",
        "def get_patch_ground_truth(patch_box, image_annotations, overlap_threshold=0.1):\n",
        "    px1, py1, px2, py2 = patch_box\n",
        "    patch_area = (px2 - px1) * (py2 - py1)\n",
        "    patch_truth = [0] * len(train_class_labels)\n",
        "    for ann in image_annotations:\n",
        "        bbox = ann['bbox']\n",
        "        bx1, by1, bw, bh = bbox; bx2, by2 = bx1 + bw, by1 + bh\n",
        "        ix1, iy1 = max(px1, bx1), max(py1, by1)\n",
        "        ix2, iy2 = min(px2, bx2), min(py2, by2)\n",
        "        inter_area = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n",
        "        if (inter_area / patch_area) > overlap_threshold:\n",
        "            class_idx = coco_id_to_class_idx.get(ann['category_id'])\n",
        "            if class_idx is not None:\n",
        "                patch_truth[class_idx] = 1\n",
        "    is_negative = 1 if sum(patch_truth) == 0 else 0\n",
        "    return patch_truth + [is_negative]\n",
        "\n",
        "\n",
        "# --- 5. MAIN LOOP: OPTIMIZED WITH BATCH PROCESSING ---\n",
        "print(f\"--- Running OPTIMIZED Sliding Window Analysis on All {len(image_id_to_filename_subset)} Images ---\")\n",
        "\n",
        "image_processing_times = []\n",
        "\n",
        "for image_id, filename in list(image_id_to_filename_subset.items())[:15]:\n",
        "    print(f\"\\n\\n=========================================================\")\n",
        "    print(f\"Processing Image: {filename}\")\n",
        "    print(f\"=============================================================\")\n",
        "    start_time = time.time() # Start timer\n",
        "\n",
        "    image_path = os.path.join(VALIDATION_DIR, filename)\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"--> SKIPPING: File not found at {image_path}\")\n",
        "        continue\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_width, image_height = image.size\n",
        "    current_image_annotations = image_id_to_annotations[image_id]\n",
        "\n",
        "    # --- OPTIMIZATION 1: Extract all patches into a list first ---\n",
        "    patches = []\n",
        "    num_patches_y = (image_height - PATCH_SIZE) // STRIDE + 1\n",
        "    num_patches_x = (image_width - PATCH_SIZE) // STRIDE + 1\n",
        "\n",
        "    for y in range(0, image_height - PATCH_SIZE + 1, STRIDE):\n",
        "        for x in range(0, image_width - PATCH_SIZE + 1, STRIDE):\n",
        "            patch = image.crop((x, y, x + PATCH_SIZE, y + PATCH_SIZE))\n",
        "            patches.append(patch)\n",
        "\n",
        "    if not patches:\n",
        "        print(\"--> SKIPPING: No patches were generated for this image.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Extracted {len(patches)} patches. Processing them in batches of {BATCH_SIZE}...\")\n",
        "\n",
        "    all_probs = []\n",
        "    image_patch_predictions = []\n",
        "\n",
        "    # --- OPTIMIZATION 2: Process the list of patches in batches ---\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(patches), BATCH_SIZE):\n",
        "            batch = patches[i:i + BATCH_SIZE]\n",
        "\n",
        "            # The processor naturally handles a list of PIL Images\n",
        "            inputs = processor(text=text_prompts, images=batch, return_tensors=\"pt\", padding=True).to(device)\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            # Get probabilities and predictions for the current batch\n",
        "            probs = outputs.logits_per_image.sigmoid()\n",
        "            predictions = (probs > CONFIDENCE_THRESHOLD).int()\n",
        "\n",
        "            all_probs.append(probs.cpu())\n",
        "            image_patch_predictions.append(predictions.cpu())\n",
        "\n",
        "    # Concatenate results from all batches\n",
        "    full_probs_tensor = torch.cat(all_probs)\n",
        "    full_predictions_tensor = torch.cat(image_patch_predictions)\n",
        "\n",
        "    # --- OPTIMIZATION 3: Reshape the results to form the heatmap ---\n",
        "    # The output order is preserved, so we can directly reshape.\n",
        "    # Reshape from (total_patches, num_classes) to (y_dim, x_dim, num_classes)\n",
        "    heatmap_tensor = full_probs_tensor.view(num_patches_y, num_patches_x, len(CLASS_LABELS))\n",
        "    # Permute to get (num_classes, y_dim, x_dim) for easy plotting\n",
        "    heatmap = heatmap_tensor.permute(2, 0, 1)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    image_processing_times.append(elapsed_time)\n",
        "    print(f\"--> Image processing finished in {end_time - start_time:.2f} seconds.\")\n",
        "\n",
        "    # --- Display Per-Image Report (Logic is the same, just using the batched results) ---\n",
        "    print(f\"\\nImage Report Card for: {filename}\")\n",
        "    print(\"-\" * 60)\n",
        "    print(f\"{'Class':<12} | {'Ground Truth Count':<20} | {'Predicted Patch Count':<22}\")\n",
        "    print(\"-\" * 60)\n",
        "    true_counts = [0] * len(train_class_labels)\n",
        "    for ann in current_image_annotations:\n",
        "        class_idx = coco_id_to_class_idx.get(ann['category_id'])\n",
        "        if class_idx is not None:\n",
        "            true_counts[class_idx] += 1\n",
        "\n",
        "    # Sum the predictions from the batched tensor result\n",
        "    predicted_counts = torch.sum(full_predictions_tensor[:, :len(train_class_labels)], axis=0).numpy()\n",
        "    for i, label in enumerate(train_class_labels):\n",
        "        print(f\"{label:<12} | {true_counts[i]:<20} | {predicted_counts[i]:<22}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    # --- Display Per-Image Heatmaps (Unchanged) ---\n",
        "    print(f\"\\nHeatmap Visualizations for: {filename}\")\n",
        "    heatmap_np = heatmap.numpy()\n",
        "    fig, axes = plt.subplots(1, len(CLASS_LABELS) + 1, figsize=(20, 5))\n",
        "\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\")\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    for i, class_name in enumerate(CLASS_LABELS):\n",
        "        im = axes[i+1].imshow(heatmap_np[i], cmap='viridis', interpolation='nearest')\n",
        "        axes[i+1].set_title(f\"Heatmap for '{class_name}'\")\n",
        "        axes[i+1].axis('off')\n",
        "\n",
        "    fig.colorbar(im, ax=axes.ravel().tolist())\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\n\\n--- Analysis of all validation images complete. ---\")\n",
        "print(f\"Average time per image: {np.mean(image_processing_times):.2f} seconds\")"
      ],
      "metadata": {
        "id": "mGvqi044wCTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oS3ZLlPmBNoj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}